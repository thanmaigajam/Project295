# -*- coding: utf-8 -*-
"""LDA_reddit_API_bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1imoD3Es_eeZbss7jt7TuhVihcA-_4k6o
"""

CLIENT_ID = 'QDeSFcgDZqq8og'
SECRET_KEY = 'xVg8u2L9Wp1IOR2NrFZ48yg_1dm8ng'

import requests



auth = requests.auth.HTTPBasicAuth(CLIENT_ID, SECRET_KEY)

auth.username

data = {
    'grant_type': 'password',
    'username': 'Judge_Normal',
    'password': 'MyTerm!05'
}

headers= {'User-Agent': 'MyAPI/0.0.1'}

res = requests.post('https://www.reddit.com/api/v1/access_token', auth=auth, data=data, headers=headers)

TOKEN = res.json()['access_token']

headers = {**headers, **{'Authorization': f'bearer {TOKEN}'}}

headers

requests.get('https://oauth.reddit.com/api/v1/me',headers = headers).json()

res1 = requests.get('https://oauth.reddit.com/r/starbucks/hot?limit=100', headers=headers)

res1.json()

for post in res1.json()['data']['children']:
  print(post['data'])

for post in res1.json()['data']['children']:
  print(post['data']['title'])

import pandas as pd

df = pd.DataFrame()

for post in res1.json()['data']['children']:
  df = df.append({
      'subeddit':post['data']['subreddit'],
      'title':post['data']['title'],
      'selftext':post['data']['selftext'],
      'upvote_ratio':post['data']['upvote_ratio'],
      'ups':post['data']['ups'],
      'downs':post['data']['downs'],
      'num_comments':post['data']['num_comments'],
      'created_utc':post['data']['created_utc'], #Check the date
      'subreddit_subscribers':post['data']['subreddit_subscribers'], 
  }, ignore_index = True)

post['data'].keys()

df

df.to_csv("reddit.csv")

df.ups.describe()

df['selftext'][23]

df['title'][23]

df['downs'].isnull().sum()

import datetime as dt

df_selftext = pd.DataFrame()

for post in res1.json()['data']['children']:
  df_selftext = df_selftext.append({
      'selftext':post['data']['selftext'], 
  }, ignore_index = True)

df_selftext

df_title = pd.DataFrame()

for post in res1.json()['data']['children']:
  df_title = df_title.append({
      'title':post['data']['title'], 
  }, ignore_index = True)

df_title

df_title.info()

#Get all the indexes of reviewtext which has url
df_title.title[df_title.title.str.match(pat = '(https)|(http)|(www.)',na = False)]

"""## Removing Emoticons"""

#!/usr/bin/env python
import re

text = u'This is a smiley face \U0001f602'
print(text) # with emoji

def deEmojify(text):
    regrex_pattern = re.compile(pattern = "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U0001f972-\U0001f972"
                           "]+", flags = re.UNICODE)
    return regrex_pattern.sub(r'',text)

print(deEmojify(text))

de_emotext = []
for text in df_title.title:
  text = deEmojify(text)
  de_emotext.append(text)

df_de_emotext = pd.DataFrame(de_emotext)

df_de_emotext

df_de_emotext[0][2]

type(list(df_selftext))

import re

de_emotext_selftext = []
for text in df_selftext.selftext:
  ##print(text)
  text = deEmojify(text)
  text_without_tags = re.sub(r'^https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)
  de_emotext_selftext.append(text_without_tags)

df_de_emotext_selftext = pd.DataFrame(de_emotext_selftext)

df_de_emotext_selftext



# pip3 install transformers

from transformers import BertTokenizer, TFBertForSequenceClassification
from transformers import InputExample, InputFeatures

model = TFBertForSequenceClassification.from_pretrained("bert-base-uncased")
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

model.summary()

import tensorflow as tf
import pandas as pd

#pred_sentences = list((df_de_emotext[0]))
pred_sentences = list((df_de_emotext_selftext[0]))

tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')
tf_outputs = model(tf_batch)
tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)
labels = ['Negative','Positive']
label = tf.argmax(tf_predictions, axis=1)
label = label.numpy()
for i in range(len(pred_sentences)):
  print(pred_sentences[i], ": ----> ", labels[label[i]])

# if __name__ == "__main__":
#     app.run(debug=True)



























